{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ReMASTER: Extension of Market-Guided Stock Transformer for Stock Price Forecasting Using Novel Stock Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Replace \"0\" with the GPU ID(s) you want to use\n",
    "\n",
    "print(f'Can I use GPU now? -- {torch.cuda.is_available()}')\n",
    "# if torch.cuda.is_available():\n",
    "#     # device = torch.device(\"cuda:0\")  # Use the first CUDA device\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"Using CUDA device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Install Required Libraries**\n",
    "Run the following commands to install necessary libraries like pandas, torch, and qlib (from GitHub):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "# install qlib library to load in datasets\n",
    "# Install pandas and torch with specific versions\n",
    "# !pip install pandas==1.5.3 torch==1.11.0\n",
    "!pip freeze > requirements.txt\n",
    "!pip install pandas==1.5.3\n",
    "!pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "\n",
    "# Install dependencies for qlib\n",
    "!pip install --upgrade cython setuptools\n",
    "\n",
    "# Install qlib from GitHub, as the PyPI version only supports Python 3.7 and 3.8\n",
    "!pip install git+https://github.com/microsoft/qlib.git\n",
    "\n",
    "# Install any remaining dependencies\n",
    "# !pip install pyqlib\n",
    "# !pip install pylib==0.9.1.99\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Import Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import qlib\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Verify GPU Availability and Set Up Data**\n",
    "Initialize qlib and check GPU status:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlib.init()\n",
    "print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Load Data from Google Drive**\n",
    "Set up the data. Load in from google drive mount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Step 1: Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Set the path to your data folder\n",
    "data_path = '/content/drive/My Drive/ECE 570/data'\n",
    "\n",
    "# Step 3: List files in the folder (to verify)\n",
    "print(\"Files in data folder:\")\n",
    "for filename in os.listdir(data_path):\n",
    "    print(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BASE_MODEL.PY**\n",
    "### **Helper Functions and Classes**\n",
    "Define the necessary functions and classes for data processing and model setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def calc_ic(pred, label):\n",
    "    df = pd.DataFrame({'pred':pred, 'label':label})\n",
    "    ic = df['pred'].corr(df['label'])\n",
    "    ric = df['pred'].corr(df['label'], method='spearman')\n",
    "    return ic, ric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Data Sampler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# MASTER'S IMPLEMENTATION (untouched)\n",
    "\n",
    "class DailyBatchSamplerRandom(Sampler):\n",
    "    def __init__(self, data_source, shuffle=False):\n",
    "        self.data_source = data_source\n",
    "        self.shuffle = shuffle\n",
    "        # calculate number of samples in each batch\n",
    "        self.daily_count = pd.Series(index=self.data_source.get_index()).groupby(\"datetime\").size().values\n",
    "        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch\n",
    "        self.daily_index[0] = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            index = np.arange(len(self.daily_count))\n",
    "            np.random.shuffle(index)\n",
    "            for i in index:\n",
    "                yield np.arange(self.daily_index[i], self.daily_index[i] + self.daily_count[i])\n",
    "        else:\n",
    "            for idx, count in zip(self.daily_index, self.daily_count):\n",
    "                yield np.arange(idx, idx + count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "# ====================================================================================================================================================================================\n",
    "#\n",
    "#                           below is experimental\n",
    "#\n",
    "# ====================================================================================================================================================================================\n",
    "\n",
    "# because nq100 is alr pandas df. added helpers as well\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# class TimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, df, lookback_window=20, num_stocks=100):  # Add num_stocks as a parameter\n",
    "#         self.df = df\n",
    "#         self.lookback_window = lookback_window\n",
    "#         self.num_stocks = num_stocks  # Store the number of stocks\n",
    "#         self.data = self._create_3d_tensor()\n",
    "\n",
    "#     def _create_3d_tensor(self):\n",
    "#         num_features = self.df.shape[1]  # Get the number of features from the DataFrame\n",
    "\n",
    "#         # Calculate the correct data_points_per_stock to ensure compatibility\n",
    "#         data_points_per_stock = len(self.df) // (self.num_stocks * num_features)\n",
    "\n",
    "#         # Reshape it to (num_stocks, data_points_per_stock, num_features)\n",
    "#         reshaped_data = self.df.values.reshape(\n",
    "#             data_points_per_stock, self.num_stocks, num_features  # Corrected order of dimensions\n",
    "#         )\n",
    "\n",
    "#         # You can convert back to a NumPy array or keep it as a tensor if needed\n",
    "#         return torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "\n",
    "# class DailyBatchSamplerRandom(torch.utils.data.Sampler):\n",
    "#     def __init__(self, dataset, shuffle=False):\n",
    "#         self.dataset = dataset  # Store the dataset object\n",
    "#         self.shuffle = shuffle\n",
    "#         self.num_stocks = dataset.num_stocks  # Access num_stocks from the dataset\n",
    "#         self.len = len(dataset) // self.num_stocks\n",
    "#         self.indices = torch.arange(self.num_stocks)\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         if self.shuffle:\n",
    "#             rand_idx = torch.randperm(self.num_stocks)\n",
    "#             self.indices = self.indices[rand_idx]\n",
    "#         for i in range(self.len):\n",
    "#             yield self.indices + i * self.num_stocks\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MASTER.PY**\n",
    "### **Model Definition**\n",
    "Define the architecture for ReMASTER model with position encoding, self-attention, and gated layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from torch.nn.modules.linear import Linear\n",
    "# from torch.nn.modules.dropout import Dropout\n",
    "# from torch.nn.modules.normalization import LayerNorm\n",
    "# import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.shape[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of SAttention, TAttention, Gate, TemporalAttention, and MASTER classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------for ReMASTER import TensorDataset and pandas------------------------------\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class SAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.temperature = math.sqrt(self.d_model/nhead)\n",
    "\n",
    "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        attn_dropout_layer = []\n",
    "        for i in range(nhead):\n",
    "            attn_dropout_layer.append(Dropout(p=dropout))\n",
    "        self.attn_dropout = nn.ModuleList(attn_dropout_layer)\n",
    "\n",
    "        # input LayerNorm\n",
    "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        # FFN layerNorm\n",
    "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            Dropout(p=dropout),\n",
    "            Linear(d_model, d_model),\n",
    "            Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        q = self.qtrans(x).transpose(0,1)\n",
    "        k = self.ktrans(x).transpose(0,1)\n",
    "        v = self.vtrans(x).transpose(0,1)\n",
    "\n",
    "        dim = int(self.d_model/self.nhead)\n",
    "        att_output = []\n",
    "        for i in range(self.nhead):\n",
    "            if i==self.nhead-1:\n",
    "                qh = q[:, :, i * dim:]\n",
    "                kh = k[:, :, i * dim:]\n",
    "                vh = v[:, :, i * dim:]\n",
    "            else:\n",
    "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
    "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
    "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
    "\n",
    "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)) / self.temperature, dim=-1)\n",
    "            if self.attn_dropout:\n",
    "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
    "            att_output.append(torch.matmul(atten_ave_matrixh, vh).transpose(0, 1))\n",
    "        att_output = torch.concat(att_output, dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        xt = x + att_output\n",
    "        xt = self.norm2(xt)\n",
    "        att_output = xt + self.ffn(xt)\n",
    "\n",
    "        return att_output\n",
    "\n",
    "\n",
    "class TAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.attn_dropout = []\n",
    "        if dropout > 0:\n",
    "            for i in range(nhead):\n",
    "                self.attn_dropout.append(Dropout(p=dropout))\n",
    "            self.attn_dropout = nn.ModuleList(self.attn_dropout)\n",
    "\n",
    "        # input LayerNorm\n",
    "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
    "        # FFN layerNorm\n",
    "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            Dropout(p=dropout),\n",
    "            Linear(d_model, d_model),\n",
    "            Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        q = self.qtrans(x)\n",
    "        k = self.ktrans(x)\n",
    "        v = self.vtrans(x)\n",
    "\n",
    "        dim = int(self.d_model / self.nhead)\n",
    "        att_output = []\n",
    "        for i in range(self.nhead):\n",
    "            if i==self.nhead-1:\n",
    "                qh = q[:, :, i * dim:]\n",
    "                kh = k[:, :, i * dim:]\n",
    "                vh = v[:, :, i * dim:]\n",
    "            else:\n",
    "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
    "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
    "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
    "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)), dim=-1)\n",
    "            if self.attn_dropout:\n",
    "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
    "            att_output.append(torch.matmul(atten_ave_matrixh, vh))\n",
    "        att_output = torch.concat(att_output, dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        xt = x + att_output\n",
    "        xt = self.norm2(xt)\n",
    "        att_output = xt + self.ffn(xt)\n",
    "\n",
    "        return att_output\n",
    "\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, d_input, d_output,  beta=1.0):\n",
    "        super().__init__()\n",
    "        self.trans = nn.Linear(d_input, d_output)\n",
    "        self.d_output =d_output\n",
    "        self.t = beta\n",
    "\n",
    "    def forward(self, gate_input):\n",
    "        output = self.trans(gate_input)\n",
    "        output = torch.softmax(output/self.t, dim=-1)\n",
    "        return self.d_output*output\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.trans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.trans(z) # [N, T, D]\n",
    "        query = h[:, -1, :].unsqueeze(-1)\n",
    "        lam = torch.matmul(h, query).squeeze(-1)  # [N, T, D] --> [N, T]\n",
    "        lam = torch.softmax(lam, dim=1).unsqueeze(1)\n",
    "        output = torch.matmul(lam, z).squeeze(1)  # [N, 1, T], [N, T, D] --> [N, 1, D]\n",
    "        return output\n",
    "\n",
    "\n",
    "class MASTER(nn.Module):\n",
    "    def __init__(self, d_feat=158, d_model=256, t_nhead=4, s_nhead=2, T_dropout_rate=0.5, S_dropout_rate=0.5,\n",
    "                 gate_input_start_index=158, gate_input_end_index=221, beta=None):\n",
    "        super(MASTER, self).__init__()\n",
    "        # market\n",
    "        self.gate_input_start_index = gate_input_start_index\n",
    "        self.gate_input_end_index = gate_input_end_index\n",
    "        self.d_gate_input = (gate_input_end_index - gate_input_start_index) # F'\n",
    "        self.feature_gate = Gate(self.d_gate_input, d_feat, beta=beta)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # feature layer\n",
    "            nn.Linear(d_feat, d_model),\n",
    "            PositionalEncoding(d_model),\n",
    "            # intra-stock aggregation\n",
    "            TAttention(d_model=d_model, nhead=t_nhead, dropout=T_dropout_rate),\n",
    "            # inter-stock aggregation\n",
    "            SAttention(d_model=d_model, nhead=s_nhead, dropout=S_dropout_rate),\n",
    "            TemporalAttention(d_model=d_model),\n",
    "            # decoder\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        src = x[:, :, :self.gate_input_start_index] # N, T, D\n",
    "        gate_input = x[:, -1, self.gate_input_start_index:self.gate_input_end_index]\n",
    "        src = src * torch.unsqueeze(self.feature_gate(gate_input), dim=1)\n",
    "\n",
    "        output = self.layers(src).squeeze(-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# class sequencemodel\n",
    "# ORIGINALLY PART OF BASE_MODEL.PY\n",
    "#   ** moved here to avoid import since this doesn't work in colab notebooks\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "class SequenceModel():\n",
    "    def __init__(self, n_epochs, lr, GPU=None, seed=None, train_stop_loss_thred=None, save_path = 'model/', save_prefix= ''):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.device = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.seed = seed\n",
    "        self.train_stop_loss_thred = train_stop_loss_thred\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        self.fitted = False\n",
    "\n",
    "        self.model = None\n",
    "        self.train_optimizer = None\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.save_prefix = save_prefix\n",
    "\n",
    "\n",
    "    def init_model(self):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"model has not been initialized\")\n",
    "\n",
    "        self.train_optimizer = optim.Adam(self.model.parameters(), self.lr)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def loss_fn(self, pred, label):\n",
    "        mask = ~torch.isnan(label)\n",
    "        loss = (pred[mask]-label[mask])**2\n",
    "        return torch.mean(loss)\n",
    "\n",
    "    def train_epoch(self, data_loader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        for data in data_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            '''\n",
    "            data.shape: (N, T, F)\n",
    "            N - number of stocks\n",
    "            T - length of lookback_window, 8\n",
    "            F - 158 factors + 63 market information + 1 label           \n",
    "            '''\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1].to(self.device)\n",
    "\n",
    "            pred = self.model(feature.float())\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.train_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)\n",
    "            self.train_optimizer.step()\n",
    "\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def test_epoch(self, data_loader):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "\n",
    "        for data in data_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1].to(self.device)\n",
    "            pred = self.model(feature.float())\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def _init_data_loader(self, data, shuffle=True, drop_last=True):\n",
    "        sampler = DailyBatchSamplerRandom(data, shuffle)\n",
    "        data_loader = DataLoader(data, sampler=sampler, drop_last=drop_last)\n",
    "        return data_loader\n",
    "\n",
    "    def load_param(self, param_path):\n",
    "        self.model.load_state_dict(torch.load(param_path, map_location=self.device))\n",
    "        self.fitted = True\n",
    "\n",
    "    def fit(self, dl_train, dl_valid):\n",
    "        train_loader = self._init_data_loader(dl_train, shuffle=True, drop_last=True)\n",
    "        valid_loader = self._init_data_loader(dl_valid, shuffle=False, drop_last=True)\n",
    "\n",
    "        self.fitted = True\n",
    "        best_param = None\n",
    "        for step in range(self.n_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.test_epoch(valid_loader)\n",
    "\n",
    "            print(\"Epoch %d, train_loss %.6f, valid_loss %.6f \" % (step, train_loss, val_loss))\n",
    "            best_param = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            if train_loss <= self.train_stop_loss_thred:\n",
    "                break\n",
    "        torch.save(best_param, f'{self.save_path}{self.save_prefix}master_{self.seed}.pkl')\n",
    "\n",
    "    def predict(self, dl_test):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"model is not fitted yet!\")\n",
    "\n",
    "        test_loader = self._init_data_loader(dl_test, shuffle=False, drop_last=False)\n",
    "\n",
    "        preds = []\n",
    "        ic = []\n",
    "        ric = []\n",
    "\n",
    "        self.model.eval()\n",
    "        for data in test_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1]\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(feature.float()).detach().cpu().numpy()\n",
    "            preds.append(pred.ravel())\n",
    "\n",
    "            daily_ic, daily_ric = calc_ic(pred, label.detach().numpy())\n",
    "            ic.append(daily_ic)\n",
    "            ric.append(daily_ric)\n",
    "\n",
    "        predictions = pd.Series(np.concatenate(preds), index=dl_test.get_index())\n",
    "\n",
    "        metrics = {\n",
    "            'IC': np.mean(ic),\n",
    "            'ICIR': np.mean(ic)/np.std(ic),\n",
    "            'RIC': np.mean(ric),\n",
    "            'RICIR': np.mean(ric)/np.std(ric)\n",
    "        }\n",
    "\n",
    "        return predictions, metrics\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# BACK TO MASTER.PY\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class MASTERModel(SequenceModel):\n",
    "    def __init__(\n",
    "            self, d_feat: int = 20, d_model: int = 64, t_nhead: int = 4, s_nhead: int = 2, gate_input_start_index=None, gate_input_end_index=None,\n",
    "            T_dropout_rate=0.5, S_dropout_rate=0.5, beta=5.0, **kwargs,\n",
    "    ):\n",
    "        super(MASTERModel, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.d_feat = d_feat\n",
    "\n",
    "        self.gate_input_start_index = gate_input_start_index\n",
    "        self.gate_input_end_index = gate_input_end_index\n",
    "\n",
    "        self.T_dropout_rate = T_dropout_rate\n",
    "        self.S_dropout_rate = S_dropout_rate\n",
    "        self.t_nhead = t_nhead\n",
    "        self.s_nhead = s_nhead\n",
    "        self.beta = beta\n",
    "\n",
    "        self.init_model()\n",
    "\n",
    "    def init_model(self):\n",
    "        self.model = MASTER(d_feat=self.d_feat, d_model=self.d_model, t_nhead=self.t_nhead, s_nhead=self.s_nhead,\n",
    "                                   T_dropout_rate=self.T_dropout_rate, S_dropout_rate=self.S_dropout_rate,\n",
    "                                   gate_input_start_index=self.gate_input_start_index,\n",
    "                                   gate_input_end_index=self.gate_input_end_index, beta=self.beta)\n",
    "        super(MASTERModel, self).init_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MAIN.PY**\n",
    "### **Model Initialization and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "universe = 'csi800' # or 'csi800'\n",
    "\n",
    "# Please install qlib first before load the data.\n",
    "with open(f'/content/drive/My Drive/ECE 570/data/{universe}/{universe}_dl_train.pkl', 'rb') as f:\n",
    "    dl_train = pickle.load(f)\n",
    "with open(f'/content/drive/My Drive/ECE 570/data/{universe}/{universe}_dl_valid.pkl', 'rb') as f:\n",
    "    dl_valid = pickle.load(f)\n",
    "with open(f'/content/drive/My Drive/ECE 570/data/{universe}/{universe}_dl_test.pkl', 'rb') as f:\n",
    "    dl_test = pickle.load(f)\n",
    "print(\"Data Loaded.\")\n",
    "\n",
    "d_feat = 158\n",
    "d_model = 256\n",
    "t_nhead = 4\n",
    "s_nhead = 2\n",
    "dropout = 0.5\n",
    "gate_input_start_index=158\n",
    "gate_input_end_index = 221\n",
    "\n",
    "if universe == 'csi300':\n",
    "    beta = 10\n",
    "elif universe == 'csi800':\n",
    "    beta = 5\n",
    "\n",
    "n_epoch = 5\n",
    "lr = 8e-6\n",
    "GPU = 0\n",
    "seed = 0\n",
    "train_stop_loss_thred = 0.95\n",
    "\n",
    "model = MASTERModel(\n",
    "    d_feat = d_feat, d_model = d_model, t_nhead = t_nhead, s_nhead = s_nhead, T_dropout_rate=dropout, S_dropout_rate=dropout,\n",
    "    beta=beta, gate_input_end_index=gate_input_end_index, gate_input_start_index=gate_input_start_index,\n",
    "    n_epochs=n_epoch, lr = lr, GPU = GPU, seed = seed, train_stop_loss_thred = train_stop_loss_thred,\n",
    "    save_path='/content/drive/My Drive/ECE 570/data/model/', save_prefix=universe\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(dl_train, dl_valid)\n",
    "print(\"Model Trained.\")\n",
    "\n",
    "# Test\n",
    "predictions, metrics = model.predict(dl_test)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Loading and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Test\n",
    "# universe\n",
    "# Step 2: Set the path to your data folder\n",
    "data_path = '/content/drive/My Drive/ECE 570/data'\n",
    "\n",
    "# Step 3: List files in the folder (to verify)\n",
    "print(\"Files in data folder:\")\n",
    "for filename in os.listdir(data_path):\n",
    "    print(filename)\n",
    "param_path = f'/content/drive/My Drive/ECE 570/model/{universe}master_0.pkl.'\n",
    "print(f'Model Loaded from {param_path}')\n",
    "model.load_param(param_path)\n",
    "predictions, metrics = model.predict(dl_test)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where the official ReMASTER ends. happy investing! :)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
