{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib as joblib\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# from ReMASTER.system import get_data_dir\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/11/2019 23:05</td>\n",
       "      <td>9073.25</td>\n",
       "      <td>9098.50</td>\n",
       "      <td>9073.00</td>\n",
       "      <td>9092.50</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/11/2019 23:10</td>\n",
       "      <td>9093.25</td>\n",
       "      <td>9095.50</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>9092.75</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/11/2019 23:15</td>\n",
       "      <td>9093.00</td>\n",
       "      <td>9096.25</td>\n",
       "      <td>9088.00</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/11/2019 23:20</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9086.00</td>\n",
       "      <td>9087.00</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/11/2019 23:25</td>\n",
       "      <td>9086.75</td>\n",
       "      <td>9088.25</td>\n",
       "      <td>9079.75</td>\n",
       "      <td>9083.75</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time     Open     High      Low    Close  Volume\n",
       "0  8/11/2019 23:05  9073.25  9098.50  9073.00  9092.50    1758\n",
       "1  8/11/2019 23:10  9093.25  9095.50  9089.75  9092.75     438\n",
       "2  8/11/2019 23:15  9093.00  9096.25  9088.00  9089.75     590\n",
       "3  8/11/2019 23:20  9090.25  9090.25  9086.00  9087.00     278\n",
       "4  8/11/2019 23:25  9086.75  9088.25  9079.75  9083.75     711"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ_5Years_8_11_2024.csv'\n",
    "file_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\NQ_5Years_8_11_2024.csv'\n",
    "\n",
    "# Initialize an empty DataFrame for the combined data\n",
    "# data = pd.DataFrame()\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to inspect its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:05:00</th>\n",
       "      <td>8/11/2019 23:05</td>\n",
       "      <td>9073.25</td>\n",
       "      <td>9098.50</td>\n",
       "      <td>9073.00</td>\n",
       "      <td>9092.50</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:10:00</th>\n",
       "      <td>8/11/2019 23:10</td>\n",
       "      <td>9093.25</td>\n",
       "      <td>9095.50</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>9092.75</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:15:00</th>\n",
       "      <td>8/11/2019 23:15</td>\n",
       "      <td>9093.00</td>\n",
       "      <td>9096.25</td>\n",
       "      <td>9088.00</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:20:00</th>\n",
       "      <td>8/11/2019 23:20</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9086.00</td>\n",
       "      <td>9087.00</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:25:00</th>\n",
       "      <td>8/11/2019 23:25</td>\n",
       "      <td>9086.75</td>\n",
       "      <td>9088.25</td>\n",
       "      <td>9079.75</td>\n",
       "      <td>9083.75</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Time     Open     High      Low    Close  \\\n",
       "Date                                                                       \n",
       "2019-08-11 23:05:00  8/11/2019 23:05  9073.25  9098.50  9073.00  9092.50   \n",
       "2019-08-11 23:10:00  8/11/2019 23:10  9093.25  9095.50  9089.75  9092.75   \n",
       "2019-08-11 23:15:00  8/11/2019 23:15  9093.00  9096.25  9088.00  9089.75   \n",
       "2019-08-11 23:20:00  8/11/2019 23:20  9090.25  9090.25  9086.00  9087.00   \n",
       "2019-08-11 23:25:00  8/11/2019 23:25  9086.75  9088.25  9079.75  9083.75   \n",
       "\n",
       "                     Volume  \n",
       "Date                         \n",
       "2019-08-11 23:05:00    1758  \n",
       "2019-08-11 23:10:00     438  \n",
       "2019-08-11 23:15:00     590  \n",
       "2019-08-11 23:20:00     278  \n",
       "2019-08-11 23:25:00     711  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert 'Date' to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# Set 'Date' as the index for time-based operations\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Verify the structure (no need to drop 'Time', as it's part of the index now)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updated method to ensure this set fits the csi300 form with alpha158 form etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reformat to fit multi-index and get the specified features that csi300 has\n",
    "\n",
    "this is what csi300's formulas were:\n",
    "\n",
    "MultiIndex([('feature',                              '($close-$open)/$open'),\n",
    "            ('feature',                                '($high-$low)/$open'),\n",
    "            ('feature',                 '($close-$open)/($high-$low+1e-12)'),\n",
    "            ('feature',              '($high-Greater($open, $close))/$open'),\n",
    "            ('feature', '($high-Greater($open, $close))/($high-$low+1e-12)'),\n",
    "            ('feature',                  '(Less($open, $close)-$low)/$open'),\n",
    "            ('feature',     '(Less($open, $close)-$low)/($high-$low+1e-12)'),\n",
    "            ('feature',                       '(2*$close-$high-$low)/$open'),\n",
    "            ('feature',          '(2*$close-$high-$low)/($high-$low+1e-12)'),\n",
    "            ('feature',                                      '$open/$close'),\n",
    "            ...\n",
    "            ('feature',          'Mask(Std($amount,20)/$amount,'SH000906')'),\n",
    "            ('feature',  'Mask(Mean($close/Ref($close,1)-1,30),'SH000906')'),\n",
    "            ('feature',   'Mask(Std($close/Ref($close,1)-1,30),'SH000906')'),\n",
    "            ('feature',         'Mask(Mean($amount,30)/$amount,'SH000906')'),\n",
    "            ('feature',          'Mask(Std($amount,30)/$amount,'SH000906')'),\n",
    "            ('feature',  'Mask(Mean($close/Ref($close,1)-1,60),'SH000906')'),\n",
    "            ('feature',   'Mask(Std($close/Ref($close,1)-1,60),'SH000906')'),\n",
    "            ('feature',         'Mask(Mean($amount,60)/$amount,'SH000906')'),\n",
    "            ('feature',          'Mask(Std($amount,60)/$amount,'SH000906')'),\n",
    "            (  'label',             'Ref($close, -5) / Ref($close, -1) - 1')],\n",
    "           length=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data structure:\n",
      "                                Time     Open     High      Low    Close  \\\n",
      "Date                                                                       \n",
      "2019-08-11 23:05:00  8/11/2019 23:05  9073.25  9098.50  9073.00  9092.50   \n",
      "2019-08-11 23:10:00  8/11/2019 23:10  9093.25  9095.50  9089.75  9092.75   \n",
      "2019-08-11 23:15:00  8/11/2019 23:15  9093.00  9096.25  9088.00  9089.75   \n",
      "2019-08-11 23:20:00  8/11/2019 23:20  9090.25  9090.25  9086.00  9087.00   \n",
      "2019-08-11 23:25:00  8/11/2019 23:25  9086.75  9088.25  9079.75  9083.75   \n",
      "\n",
      "                     Volume  \n",
      "Date                         \n",
      "2019-08-11 23:05:00    1758  \n",
      "2019-08-11 23:10:00     438  \n",
      "2019-08-11 23:15:00     590  \n",
      "2019-08-11 23:20:00     278  \n",
      "2019-08-11 23:25:00     711  \n",
      "\n",
      "Input data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 329458 entries, 2019-08-11 23:05:00 to 2024-08-09 16:55:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    329458 non-null  object \n",
      " 1   Open    329458 non-null  float64\n",
      " 2   High    329458 non-null  float64\n",
      " 3   Low     329458 non-null  float64\n",
      " 4   Close   329458 non-null  float64\n",
      " 5   Volume  329458 non-null  int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 17.6+ MB\n",
      "None\n",
      "Price Change Ratios done\n",
      "Moving Averages and Standard Deviations done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirani\\AppData\\Local\\Temp\\5\\ipykernel_24112\\1886209016.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  lambda x: linregress(range(len(x)), x).slope / x[-1] if len(x) == window else np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Strength and Trends done\n",
      "Volume-Weighted Measures done\n",
      "Historical Close Ratios done\n",
      "Correlation Measures done\n",
      "Price Extremes and Ratios done\n",
      "\n",
      "Sample of training data:\n",
      "                       (close-open)/open  (high-low)/open  \\\n",
      "datetime   instrument                                       \n",
      "2019-08-11 NQ100                0.002122         0.002810   \n",
      "           NQ100               -0.000055         0.000632   \n",
      "           NQ100               -0.000357         0.000907   \n",
      "           NQ100               -0.000358         0.000468   \n",
      "           NQ100               -0.000330         0.000935   \n",
      "\n",
      "                       (2*close-high-low)/open  Mean(close,5)/close  \\\n",
      "datetime   instrument                                                 \n",
      "2019-08-11 NQ100                      0.001488                  NaN   \n",
      "           NQ100                      0.000027                  NaN   \n",
      "           NQ100                     -0.000522                  NaN   \n",
      "           NQ100                     -0.000248                  NaN   \n",
      "           NQ100                     -0.000055             1.000594   \n",
      "\n",
      "                       Mean(close,10)/close  Mean(close,20)/close  \\\n",
      "datetime   instrument                                               \n",
      "2019-08-11 NQ100                        NaN                   NaN   \n",
      "           NQ100                        NaN                   NaN   \n",
      "           NQ100                        NaN                   NaN   \n",
      "           NQ100                        NaN                   NaN   \n",
      "           NQ100                        NaN                   NaN   \n",
      "\n",
      "                       Std(close,5)/close  Std(close,10)/close  \\\n",
      "datetime   instrument                                            \n",
      "2019-08-11 NQ100                      NaN                  NaN   \n",
      "           NQ100                      NaN                  NaN   \n",
      "           NQ100                      NaN                  NaN   \n",
      "           NQ100                      NaN                  NaN   \n",
      "           NQ100                  0.00042                  NaN   \n",
      "\n",
      "                       Std(close,20)/close  Slope(close,5)/close  ...  \\\n",
      "datetime   instrument                                             ...   \n",
      "2019-08-11 NQ100                       NaN                   NaN  ...   \n",
      "           NQ100                       NaN                   NaN  ...   \n",
      "           NQ100                       NaN                   NaN  ...   \n",
      "           NQ100                       NaN                   NaN  ...   \n",
      "           NQ100                       NaN             -0.000256  ...   \n",
      "\n",
      "                       VWAP/close  Mean(volume,5)/(volume+1e-12)  \\\n",
      "datetime   instrument                                              \n",
      "2019-08-11 NQ100         1.000000                            NaN   \n",
      "           NQ100         0.999978                            NaN   \n",
      "           NQ100         1.000243                            NaN   \n",
      "           NQ100         1.000496                            NaN   \n",
      "           NQ100         1.000693                       1.061885   \n",
      "\n",
      "                       Std(volume,5)/(volume+1e-12)  Ref(close,5)/close  \\\n",
      "datetime   instrument                                                     \n",
      "2019-08-11 NQ100                                NaN                 NaN   \n",
      "           NQ100                                NaN                 NaN   \n",
      "           NQ100                                NaN                 NaN   \n",
      "           NQ100                                NaN                 NaN   \n",
      "           NQ100                           0.821066                 NaN   \n",
      "\n",
      "                       Ref(close,10)/close  Ref(close,20)/close  \\\n",
      "datetime   instrument                                             \n",
      "2019-08-11 NQ100                       NaN                  NaN   \n",
      "           NQ100                       NaN                  NaN   \n",
      "           NQ100                       NaN                  NaN   \n",
      "           NQ100                       NaN                  NaN   \n",
      "           NQ100                       NaN                  NaN   \n",
      "\n",
      "                       Corr(close,log(volume+1),5)  \\\n",
      "datetime   instrument                                \n",
      "2019-08-11 NQ100                               NaN   \n",
      "           NQ100                               NaN   \n",
      "           NQ100                               NaN   \n",
      "           NQ100                               NaN   \n",
      "           NQ100                           0.30595   \n",
      "\n",
      "                       Corr(close/Ref(close,1),log(volume/Ref(volume,1)+1),5)  \\\n",
      "datetime   instrument                                                           \n",
      "2019-08-11 NQ100                                                     NaN        \n",
      "           NQ100                                                     NaN        \n",
      "           NQ100                                                     NaN        \n",
      "           NQ100                                                     NaN        \n",
      "           NQ100                                                     NaN        \n",
      "\n",
      "                       Max(high,5)/close  Min(low,5)/close  \n",
      "datetime   instrument                                       \n",
      "2019-08-11 NQ100                     NaN               NaN  \n",
      "           NQ100                     NaN               NaN  \n",
      "           NQ100                     NaN               NaN  \n",
      "           NQ100                     NaN               NaN  \n",
      "           NQ100                1.001624          0.998817  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Dataset shapes:\n",
      "Train data: (171002, 22)\n",
      "Validation data: (15987, 22)\n",
      "Test data: (142469, 22)\n"
     ]
    }
   ],
   "source": [
    "# claude version\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def calculate_csi300_features(df, instrument_id='NQ100'):\n",
    "    \"\"\"Calculate features and reshape data into CSI300 format.\"\"\"\n",
    "    # Make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create features dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Price Change Ratios\n",
    "    features['(close-open)/open'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    features['(high-low)/open'] = (df['High'] - df['Low']) / df['Open']\n",
    "    features['(2*close-high-low)/open'] = (2 * df['Close'] - df['High'] - df['Low']) / df['Open']\n",
    "    print(\"Price Change Ratios done\")\n",
    "\n",
    "    # Moving Averages and Standard Deviations (Volatility)\n",
    "    features['Mean(close,5)/close'] = df['Close'].rolling(window=5).mean() / df['Close']\n",
    "    features['Mean(close,10)/close'] = df['Close'].rolling(window=10).mean() / df['Close']\n",
    "    features['Mean(close,20)/close'] = df['Close'].rolling(window=20).mean() / df['Close']\n",
    "    features['Std(close,5)/close'] = df['Close'].rolling(window=5).std() / df['Close']\n",
    "    features['Std(close,10)/close'] = df['Close'].rolling(window=10).std() / df['Close']\n",
    "    features['Std(close,20)/close'] = df['Close'].rolling(window=20).std() / df['Close']\n",
    "    print(\"Moving Averages and Standard Deviations done\")\n",
    "\n",
    "    # Relative Strength and Trends (Slope calculation)\n",
    "    for window in [5, 10, 20]:\n",
    "        features[f'Slope(close,{window})/close'] = df['Close'].rolling(window).apply(\n",
    "            lambda x: linregress(range(len(x)), x).slope / x[-1] if len(x) == window else np.nan\n",
    "        )\n",
    "    print(\"Relative Strength and Trends done\")\n",
    "\n",
    "    # Volume-Weighted Measures\n",
    "    features['VWAP/close'] = (df['Volume'] * df['Close']).cumsum() / df['Volume'].cumsum() / df['Close']\n",
    "    features['Mean(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).mean() / (df['Volume'] + 1e-12)\n",
    "    features['Std(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).std() / (df['Volume'] + 1e-12)\n",
    "    print(\"Volume-Weighted Measures done\")\n",
    "\n",
    "    # Historical Close Ratios\n",
    "    features['Ref(close,5)/close'] = df['Close'].shift(5) / df['Close']\n",
    "    features['Ref(close,10)/close'] = df['Close'].shift(10) / df['Close']\n",
    "    features['Ref(close,20)/close'] = df['Close'].shift(20) / df['Close']\n",
    "    print(\"Historical Close Ratios done\")\n",
    "\n",
    "    # Correlation Measures\n",
    "    features['Corr(close,log(volume+1),5)'] = df['Close'].rolling(window=5).corr(np.log(df['Volume'] + 1))\n",
    "    features['Corr(close/Ref(close,1),log(volume/Ref(volume,1)+1),5)'] = (\n",
    "        (df['Close'] / df['Close'].shift(1)).rolling(window=5)\n",
    "        .corr(np.log(df['Volume'] / df['Volume'].shift(1) + 1))\n",
    "    )\n",
    "    print(\"Correlation Measures done\")\n",
    "\n",
    "    # Price Extremes and Ratios\n",
    "    features['Max(high,5)/close'] = df['High'].rolling(window=5).max() / df['Close']\n",
    "    features['Min(low,5)/close'] = df['Low'].rolling(window=5).min() / df['Close']\n",
    "    print(\"Price Extremes and Ratios done\")\n",
    "\n",
    "    # Create DataFrame from features with the same index as input data\n",
    "    feature_df = pd.DataFrame(features, index=df.index)\n",
    "\n",
    "    # Add datetime (using the index date) and instrument columns\n",
    "    feature_df['datetime'] = feature_df.index.date\n",
    "    feature_df['instrument'] = instrument_id\n",
    "\n",
    "    # Reorder columns to put datetime and instrument first\n",
    "    cols = ['datetime', 'instrument'] + [col for col in feature_df.columns if col not in ['datetime', 'instrument']]\n",
    "    feature_df = feature_df[cols]\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "def process_single_instrument(data, instrument_id, date_ranges):\n",
    "    \"\"\"Process data for a single instrument.\"\"\"\n",
    "    # Calculate features\n",
    "    processed_df = calculate_csi300_features(data, instrument_id)\n",
    "\n",
    "    # Set datetime and instrument as index\n",
    "    processed_df.set_index(['datetime', 'instrument'], inplace=True)\n",
    "\n",
    "    # Split data based on date ranges\n",
    "    train_data = processed_df[processed_df.index.get_level_values(0) <= date_ranges['train_end'].date()]\n",
    "    valid_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['valid_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['valid_end'].date())\n",
    "    ]\n",
    "    test_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['test_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['test_end'].date())\n",
    "    ]\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "# Debug: Print input data structure\n",
    "print(\"Input data structure:\")\n",
    "print(data.head())\n",
    "print(\"\\nInput data info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Define date ranges\n",
    "date_ranges = {\n",
    "    'train_end': datetime(2022, 3, 31),\n",
    "    'valid_start': datetime(2022, 4, 1),\n",
    "    'valid_end': datetime(2022, 6, 30),\n",
    "    'test_start': datetime(2022, 7, 1),\n",
    "    'test_end': datetime(2024, 12, 31)\n",
    "}\n",
    "\n",
    "# Process the data for NQ100\n",
    "instrument_id = 'NQ100'\n",
    "train_data, valid_data, test_data = process_single_instrument(data, instrument_id, date_ranges)\n",
    "\n",
    "# Save the processed datasets\n",
    "train_data.to_csv('train_data_2020_2022_NQ100.csv')\n",
    "valid_data.to_csv('valid_data_2022_NQ100.csv')\n",
    "test_data.to_csv('test_data_2022_2024_NQ100.csv')\n",
    "\n",
    "# Print sample output to verify format\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Print shape information\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Validation data: {valid_data.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_train.pkl',\n",
       " 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_valid.pkl',\n",
       " 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_test.pkl')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Save each set as .pkl files\n",
    "# # locally\n",
    "# train_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_train.pkl'\n",
    "# valid_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_valid.pkl'\n",
    "# test_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# # google drive for colab\n",
    "# train_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_train.pkl'\n",
    "# valid_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_valid.pkl'\n",
    "# test_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# onedrive when working at krach\n",
    "train_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_train.pkl'\n",
    "valid_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_valid.pkl'\n",
    "test_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_test.pkl'\n",
    "\n",
    "# Save train data\n",
    "with open(train_path, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "# Save validation data\n",
    "with open(valid_path, 'wb') as f:\n",
    "    pickle.dump(valid_data, f)\n",
    "\n",
    "# Save test data\n",
    "with open(test_path, 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "# Return the paths (optional)\n",
    "train_path, valid_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensure pkl files have content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exists in C:\\Users\\amirani\\ReMASTER\\data\\NQ100\\NQ100_dl_train.pkl and has 171002 rows and 22 columns.\n",
      "Data exists in C:\\Users\\amirani\\ReMASTER\\data\\NQ100\\NQ100_dl_valid.pkl and has 15987 rows and 22 columns.\n",
      "Data exists in C:\\Users\\amirani\\ReMASTER\\data\\NQ100\\NQ100_dl_test.pkl and has 142469 rows and 22 columns.\n"
     ]
    }
   ],
   "source": [
    "# Function to load and test if data exists in each .pkl file\n",
    "def test_data_in_pkl(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        # Check if the DataFrame is not empty\n",
    "        if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "            print(f\"Data exists in {file_path} and has {len(data)} rows and {len(data.columns)} columns.\")\n",
    "        else:\n",
    "            print(f\"File {file_path} is either empty or not a DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Test each .pkl file\n",
    "test_data_in_pkl(train_path)\n",
    "test_data_in_pkl(valid_path)\n",
    "test_data_in_pkl(test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Convert 'Time' column to datetime format\n",
    "# data['Time'] = pd.to_datetime(data['Time'], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "# # Step 2: Check for any NA values and drop columns with NA values (if any)\n",
    "# data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# # Step 3: Perform robust daily Z-score normalization on each feature dimension\n",
    "# # Extract date component for daily grouping\n",
    "# data['Date'] = data['Time'].dt.date\n",
    "# # Group by 'Date' and normalize 'Open', 'High', 'Low', 'Close', and 'Volume' columns\n",
    "# feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "# data[feature_columns] = data.groupby('Date')[feature_columns].transform(zscore)\n",
    "\n",
    "# # Step 4: Drop 5% of the most extreme values from the 'Close' column to reduce label outliers\n",
    "# # Identify upper and lower 2.5% quantiles for 'Close'\n",
    "# q_low, q_high = data['Close'].quantile([0.025, 0.975])\n",
    "# data = data[(data['Close'] >= q_low) & (data['Close'] <= q_high)]\n",
    "\n",
    "# # Step 5: Drop NA rows (if any remain after filtering) and reset index\n",
    "# data.dropna(inplace=True)\n",
    "# data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Step 6: Save the cleaned data to a new CSV file\n",
    "# # Create the directory if it doesn't exist\n",
    "# save_path = 'new_data/reshaped_data.csv'\n",
    "# os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "# data.to_csv(save_path, index=False)\n",
    "\n",
    "# # Display first few rows of the cleaned data to confirm\n",
    "# data.head()\n",
    "\n",
    "# print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, valid, and test sets by datetime. Earlier years are in training, recent years in test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'data' is your original dataframe with columns like 'Date', 'Time', 'Open', 'High', 'Low', 'Close', etc.\n",
    "\n",
    "# # 1. Combine Date and Time into a single 'datetime' column\n",
    "# # Ensure that 'Date' is a string or datetime type and 'Time' is a string (e.g., \"HH:MM:SS\")\n",
    "\n",
    "# # Convert 'Date' to datetime (if not already in datetime format)\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# # Check the type of 'Time' column to handle it accordingly\n",
    "# if data['Time'].dtype == 'datetime64[ns]':  # If Time is already in datetime64[ns]\n",
    "#     data['Time'] = data['Time'].dt.time  # Extract the time part only\n",
    "# else:  # If Time is a string like 'HH:MM:SS'\n",
    "#     data['Time'] = pd.to_timedelta(data['Time'], errors='coerce')\n",
    "\n",
    "# # Combine 'Date' and 'Time' into a single 'datetime' column\n",
    "# data['datetime'] = data['Date'] + pd.to_timedelta(data['Time'].astype(str))\n",
    "\n",
    "# # Drop the original 'Date' and 'Time' columns if no longer needed\n",
    "# data.drop(columns=['Date', 'Time'], inplace=True)\n",
    "\n",
    "# # 2. Feature Engineering\n",
    "# data['close_open'] = (data['Close'] - data['Open']) / data['Open']\n",
    "# data['high_low'] = (data['High'] - data['Low']) / data['Open']\n",
    "# data['close_open_highlow'] = (data['Close'] - data['Open']) / (data['High'] - data['Low'] + 1e-12)\n",
    "# data['high_max_open_close'] = (data['High'] - data[['Open', 'Close']].max(axis=1)) / data['Open']\n",
    "\n",
    "# # 3. Handle NaN or infinite values\n",
    "# # Replace NaN with the column mean, or you can use other strategies\n",
    "# data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# # Replace any infinity values with a large number or NaN\n",
    "# data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# # Reapply fillna to handle any NaNs created by infinities\n",
    "# data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# # 4. Normalize the newly created features\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Select columns for scaling\n",
    "# features = ['close_open', 'high_low', 'close_open_highlow', 'high_max_open_close']\n",
    "# data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# # 5. Split the data into train, valid, and test\n",
    "# train_size = int(0.7 * len(data))  # 70% for training\n",
    "# valid_size = int(0.15 * len(data))  # 15% for validation\n",
    "# test_size = len(data) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "# train_data = data[:train_size]\n",
    "# valid_data = data[train_size:train_size + valid_size]\n",
    "# test_data = data[train_size + valid_size:]\n",
    "\n",
    "# # 6. Save each set as .pkl files\n",
    "# train_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_train.pkl'\n",
    "# valid_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_valid.pkl'\n",
    "# test_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# # Save train data\n",
    "# with open(train_path, 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "\n",
    "# # Save validation data\n",
    "# with open(valid_path, 'wb') as f:\n",
    "#     pickle.dump(valid_data, f)\n",
    "\n",
    "# # Save test data\n",
    "# with open(test_path, 'wb') as f:\n",
    "#     pickle.dump(test_data, f)\n",
    "\n",
    "# # Return the paths (optional)\n",
    "# train_path, valid_path, test_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure data is in the .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to load and test if data exists in each .pkl file\n",
    "# def test_data_in_pkl(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             data = pickle.load(f)\n",
    "#         # Check if the DataFrame is not empty\n",
    "#         if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "#             print(f\"Data exists in {file_path} and has {len(data)} rows and {len(data.columns)} columns.\")\n",
    "#         else:\n",
    "#             print(f\"File {file_path} is either empty or not a DataFrame.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# # Test each .pkl file\n",
    "# test_data_in_pkl(train_path)\n",
    "# test_data_in_pkl(valid_path)\n",
    "# test_data_in_pkl(test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
