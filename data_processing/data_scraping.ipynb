{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib as joblib\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# from ReMASTER.system import get_data_dir\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/11/2019 23:05</td>\n",
       "      <td>9073.25</td>\n",
       "      <td>9098.50</td>\n",
       "      <td>9073.00</td>\n",
       "      <td>9092.50</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/11/2019 23:10</td>\n",
       "      <td>9093.25</td>\n",
       "      <td>9095.50</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>9092.75</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/11/2019 23:15</td>\n",
       "      <td>9093.00</td>\n",
       "      <td>9096.25</td>\n",
       "      <td>9088.00</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/11/2019 23:20</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9086.00</td>\n",
       "      <td>9087.00</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/11/2019 23:25</td>\n",
       "      <td>9086.75</td>\n",
       "      <td>9088.25</td>\n",
       "      <td>9079.75</td>\n",
       "      <td>9083.75</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time     Open     High      Low    Close  Volume\n",
       "0  8/11/2019 23:05  9073.25  9098.50  9073.00  9092.50    1758\n",
       "1  8/11/2019 23:10  9093.25  9095.50  9089.75  9092.75     438\n",
       "2  8/11/2019 23:15  9093.00  9096.25  9088.00  9089.75     590\n",
       "3  8/11/2019 23:20  9090.25  9090.25  9086.00  9087.00     278\n",
       "4  8/11/2019 23:25  9086.75  9088.25  9079.75  9083.75     711"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ_5Years_8_11_2024.csv'\n",
    "file_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\NQ_5Years_8_11_2024.csv'\n",
    "\n",
    "# Initialize an empty DataFrame for the combined data\n",
    "# data = pd.DataFrame()\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to inspect its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:05:00</th>\n",
       "      <td>8/11/2019 23:05</td>\n",
       "      <td>9073.25</td>\n",
       "      <td>9098.50</td>\n",
       "      <td>9073.00</td>\n",
       "      <td>9092.50</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:10:00</th>\n",
       "      <td>8/11/2019 23:10</td>\n",
       "      <td>9093.25</td>\n",
       "      <td>9095.50</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>9092.75</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:15:00</th>\n",
       "      <td>8/11/2019 23:15</td>\n",
       "      <td>9093.00</td>\n",
       "      <td>9096.25</td>\n",
       "      <td>9088.00</td>\n",
       "      <td>9089.75</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:20:00</th>\n",
       "      <td>8/11/2019 23:20</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9090.25</td>\n",
       "      <td>9086.00</td>\n",
       "      <td>9087.00</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-11 23:25:00</th>\n",
       "      <td>8/11/2019 23:25</td>\n",
       "      <td>9086.75</td>\n",
       "      <td>9088.25</td>\n",
       "      <td>9079.75</td>\n",
       "      <td>9083.75</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Time     Open     High      Low    Close  \\\n",
       "Date                                                                       \n",
       "2019-08-11 23:05:00  8/11/2019 23:05  9073.25  9098.50  9073.00  9092.50   \n",
       "2019-08-11 23:10:00  8/11/2019 23:10  9093.25  9095.50  9089.75  9092.75   \n",
       "2019-08-11 23:15:00  8/11/2019 23:15  9093.00  9096.25  9088.00  9089.75   \n",
       "2019-08-11 23:20:00  8/11/2019 23:20  9090.25  9090.25  9086.00  9087.00   \n",
       "2019-08-11 23:25:00  8/11/2019 23:25  9086.75  9088.25  9079.75  9083.75   \n",
       "\n",
       "                     Volume  \n",
       "Date                         \n",
       "2019-08-11 23:05:00    1758  \n",
       "2019-08-11 23:10:00     438  \n",
       "2019-08-11 23:15:00     590  \n",
       "2019-08-11 23:20:00     278  \n",
       "2019-08-11 23:25:00     711  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert 'Date' to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# Set 'Date' as the index for time-based operations\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Verify the structure (no need to drop 'Time', as it's part of the index now)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updated method to ensure this set fits the csi300 form with alpha158 form etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "\n",
    "# # 1. Convert 'Date' to datetime format\n",
    "# data['Date'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# # Set 'Date' as the index for time-based operations\n",
    "# data.set_index('Date', inplace=True)\n",
    "\n",
    "# # 2. Define the date ranges for the new splits (2020-2024)\n",
    "# train_end = datetime(2022, 3, 31)  # Q1 2022 ends\n",
    "# valid_start = datetime(2022, 4, 1)  # Q2 2022 starts\n",
    "# valid_end = datetime(2022, 6, 30)  # Q2 2022 ends\n",
    "# test_start = datetime(2022, 7, 1)  # Q3 2022 starts\n",
    "# test_end = datetime(2024, 12, 31)  # Q4 2024 ends\n",
    "# # \n",
    "# # 3. Create train, validation, and test sets based on the new splits\n",
    "# train_data = data[data.index <= train_end]\n",
    "# valid_data = data[(data.index >= valid_start) & (data.index <= valid_end)]\n",
    "# test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
    "\n",
    "# # 4. Feature Engineering - Add Alpha158-like features\n",
    "# def calculate_alpha158_features(df):\n",
    "#     df['close_open'] = (df['Close'] - df['Open']) / df['Open']  # Close to Open ratio\n",
    "#     df['high_low'] = (df['High'] - df['Low']) / df['Open']  # High-Low to Open ratio\n",
    "#     df['close_open_highlow'] = (df['Close'] - df['Open']) / (df['High'] - df['Low'] + 1e-12)  # Close-Open to High-Low ratio\n",
    "#     df['high_max_open_close'] = (df['High'] - np.maximum(df['Open'], df['Close'])) / df['Open']  # High minus max(Open, Close) to Open ratio\n",
    "#     df['volume_change'] = df['Volume'].pct_change()  # Percentage change in volume\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering to each dataset\n",
    "# train_data = calculate_alpha158_features(train_data)\n",
    "# valid_data = calculate_alpha158_features(valid_data)\n",
    "# test_data = calculate_alpha158_features(test_data)\n",
    "\n",
    "# # 5. Lookback Window and Prediction Interval\n",
    "# lookback_window = 8  # 8 days lookback window\n",
    "# prediction_interval = 5  # 5 days prediction interval\n",
    "\n",
    "# def create_lookback_features(df, lookback_window, prediction_interval):\n",
    "#     df['ma_close'] = df['Close'].rolling(window=lookback_window).mean()  # Moving average of the closing price\n",
    "#     df['ma_volume'] = df['Volume'].rolling(window=lookback_window).mean()  # Moving average of volume\n",
    "#     df['price_change'] = df['Close'].pct_change(periods=prediction_interval)  # Percentage change in closing price over the prediction interval\n",
    "#     df['volatility'] = df['Close'].rolling(window=lookback_window).std()  # Standard deviation (volatility) over the lookback window\n",
    "#     return df\n",
    "\n",
    "# # Apply the lookback features to each dataset\n",
    "# train_data = create_lookback_features(train_data, lookback_window, prediction_interval)\n",
    "# valid_data = create_lookback_features(valid_data, lookback_window, prediction_interval)\n",
    "# test_data = create_lookback_features(test_data, lookback_window, prediction_interval)\n",
    "\n",
    "# # 6. Optional: Handle missing data (NaN) in the feature columns\n",
    "# train_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "# valid_data.fillna(method='ffill', inplace=True)\n",
    "# test_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# # 7. Optional: Scaling the features\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# features = ['close_open', 'high_low', 'close_open_highlow', 'high_max_open_close', 'volume_change', 'ma_close', 'ma_volume', 'price_change', 'volatility']\n",
    "# train_data[features] = scaler.fit_transform(train_data[features])\n",
    "# valid_data[features] = scaler.transform(valid_data[features])\n",
    "# test_data[features] = scaler.transform(test_data[features])\n",
    "\n",
    "# # 8. Save the processed datasets\n",
    "# train_data.to_csv('train_data_2020_2022.csv')\n",
    "# valid_data.to_csv('valid_data_2022.csv')\n",
    "# test_data.to_csv('test_data_2022_2024.csv')\n",
    "\n",
    "# # 9. Show the final processed data\n",
    "# print(\"Training Set:\")\n",
    "# print(train_data.head())\n",
    "# # print(\"Validation Set:\")\n",
    "# # print(valid_data.head())\n",
    "# # print(\"Test Set:\")\n",
    "# # print(test_data.head())\n",
    "\n",
    "# #10. check column names\n",
    "# print(\"training set: \")\n",
    "# print(train_data.columns)\n",
    "# # print(\"validation set: \")\n",
    "# # print(valid_data.columns)\n",
    "# # print(\"test set: \")\n",
    "# # print(test_data.columns)\n",
    "\n",
    "# #10. check datatypes\n",
    "# print(\"training set: \")\n",
    "# print(train_data.dtypes)\n",
    "# # print(\"validation set: \")\n",
    "# # print(valid_data.dtypes)\n",
    "# # print(\"test set: \")\n",
    "# # print(test_data.dtypes)\n",
    "\n",
    "# #10. check shape (len, width) \n",
    "# print(\"training set: \")\n",
    "# print(train_data.shape)\n",
    "# # print(\"validation set: \")\n",
    "# # print(valid_data.shape)\n",
    "# # print(\"test set: \")\n",
    "# # print(test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reformat to fit multi-index and get the specified features that csi300 has\n",
    "\n",
    "this is what csi300's formulas were:\n",
    "\n",
    "MultiIndex([('feature',                              '($close-$open)/$open'),\n",
    "            ('feature',                                '($high-$low)/$open'),\n",
    "            ('feature',                 '($close-$open)/($high-$low+1e-12)'),\n",
    "            ('feature',              '($high-Greater($open, $close))/$open'),\n",
    "            ('feature', '($high-Greater($open, $close))/($high-$low+1e-12)'),\n",
    "            ('feature',                  '(Less($open, $close)-$low)/$open'),\n",
    "            ('feature',     '(Less($open, $close)-$low)/($high-$low+1e-12)'),\n",
    "            ('feature',                       '(2*$close-$high-$low)/$open'),\n",
    "            ('feature',          '(2*$close-$high-$low)/($high-$low+1e-12)'),\n",
    "            ('feature',                                      '$open/$close'),\n",
    "            ...\n",
    "            ('feature',          'Mask(Std($amount,20)/$amount,'SH000906')'),\n",
    "            ('feature',  'Mask(Mean($close/Ref($close,1)-1,30),'SH000906')'),\n",
    "            ('feature',   'Mask(Std($close/Ref($close,1)-1,30),'SH000906')'),\n",
    "            ('feature',         'Mask(Mean($amount,30)/$amount,'SH000906')'),\n",
    "            ('feature',          'Mask(Std($amount,30)/$amount,'SH000906')'),\n",
    "            ('feature',  'Mask(Mean($close/Ref($close,1)-1,60),'SH000906')'),\n",
    "            ('feature',   'Mask(Std($close/Ref($close,1)-1,60),'SH000906')'),\n",
    "            ('feature',         'Mask(Mean($amount,60)/$amount,'SH000906')'),\n",
    "            ('feature',          'Mask(Std($amount,60)/$amount,'SH000906')'),\n",
    "            (  'label',             'Ref($close, -5) / Ref($close, -1) - 1')],\n",
    "           length=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # 1. Convert 'Date' to datetime format\n",
    "# data['Date'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# # Set 'Date' as the index for time-based operations\n",
    "# data.set_index('Date', inplace=True)\n",
    "\n",
    "# # 2. Define the date ranges for the new splits (2020-2024)\n",
    "# train_end = datetime(2022, 3, 31)  # Q1 2022 ends\n",
    "# valid_start = datetime(2022, 4, 1)  # Q2 2022 starts\n",
    "# valid_end = datetime(2022, 6, 30)  # Q2 2022 ends\n",
    "# test_start = datetime(2022, 7, 1)  # Q3 2022 starts\n",
    "# test_end = datetime(2024, 12, 31)  # Q4 2024 ends\n",
    "\n",
    "# # 3. Create train, validation, and test sets based on the new splits\n",
    "# train_data = data[data.index <= train_end]\n",
    "# valid_data = data[(data.index >= valid_start) & (data.index <= valid_end)]\n",
    "# test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
    "\n",
    "# # 4. Feature Engineering - Add Alpha158-like features (more extensive)\n",
    "# def calculate_alpha158_features(df):\n",
    "#     # Basic features\n",
    "#     df['close_open'] = (df['Close'] - df['Open']) / df['Open']  # Close to Open ratio\n",
    "#     df['high_low'] = (df['High'] - df['Low']) / df['Open']  # High-Low to Open ratio\n",
    "#     df['close_open_highlow'] = (df['Close'] - df['Open']) / (df['High'] - df['Low'] + 1e-12)  # Close-Open to High-Low ratio\n",
    "#     df['high_max_open_close'] = (df['High'] - np.maximum(df['Open'], df['Close'])) / df['Open']  # High minus max(Open, Close) to Open ratio\n",
    "#     df['volume_change'] = df['Volume'].pct_change()  # Percentage change in volume\n",
    "    \n",
    "#     # Additional advanced features (based on CSI300)\n",
    "#     df['high_minus_low'] = df['High'] - df['Low']  # High-Low difference\n",
    "#     df['low_max_open_close'] = df['Low'] - np.maximum(df['Open'], df['Close'])  # Low minus max(Open, Close)\n",
    "#     df['high_to_open'] = (df['High'] - df['Open']) / df['Open']  # High to Open ratio\n",
    "#     df['low_to_open'] = (df['Low'] - df['Open']) / df['Open']  # Low to Open ratio\n",
    "#     df['close_to_high'] = (df['Close'] - df['High']) / df['High']  # Close to High ratio\n",
    "#     df['close_to_low'] = (df['Close'] - df['Low']) / df['Low']  # Close to Low ratio\n",
    "\n",
    "#     # Rolling features (Moving Averages and Volatility)\n",
    "#     df['ma_close'] = df['Close'].rolling(window=8).mean()  # 8-day moving average of Close\n",
    "#     df['ma_volume'] = df['Volume'].rolling(window=8).mean()  # 8-day moving average of Volume\n",
    "#     df['volatility'] = df['Close'].rolling(window=8).std()  # 8-day rolling standard deviation (volatility)\n",
    "#     df['price_change'] = df['Close'].pct_change(periods=5)  # 5-day percentage change in Close\n",
    "\n",
    "#     # More moving averages (longer periods)\n",
    "#     df['ma_30_close'] = df['Close'].rolling(window=30).mean()  # 30-day moving average of Close\n",
    "#     df['ma_60_close'] = df['Close'].rolling(window=60).mean()  # 60-day moving average of Close\n",
    "#     df['ma_30_volume'] = df['Volume'].rolling(window=30).mean()  # 30-day moving average of Volume\n",
    "\n",
    "#     # Adding additional feature like a ratio involving rolling standard deviation\n",
    "#     df['std_close'] = df['Close'].rolling(window=30).std()  # 30-day rolling standard deviation\n",
    "#     df['std_volume'] = df['Volume'].rolling(window=30).std()  # 30-day rolling volume standard deviation\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering to each dataset\n",
    "# train_data = calculate_alpha158_features(train_data)\n",
    "# valid_data = calculate_alpha158_features(valid_data)\n",
    "# test_data = calculate_alpha158_features(test_data)\n",
    "\n",
    "# # 5. Lookback Window and Prediction Interval (for future prediction, if needed)\n",
    "# lookback_window = 8  # 8 days lookback window\n",
    "# prediction_interval = 5  # 5 days prediction interval\n",
    "\n",
    "# def create_lookback_features(df, lookback_window, prediction_interval):\n",
    "#     df['ma_close'] = df['Close'].rolling(window=lookback_window).mean()  # Moving average of the closing price\n",
    "#     df['ma_volume'] = df['Volume'].rolling(window=lookback_window).mean()  # Moving average of volume\n",
    "#     df['price_change'] = df['Close'].pct_change(periods=prediction_interval)  # Percentage change in closing price over the prediction interval\n",
    "#     df['volatility'] = df['Close'].rolling(window=lookback_window).std()  # Standard deviation (volatility) over the lookback window\n",
    "#     return df\n",
    "\n",
    "# # Apply the lookback features to each dataset\n",
    "# train_data = create_lookback_features(train_data, lookback_window, prediction_interval)\n",
    "# valid_data = create_lookback_features(valid_data, lookback_window, prediction_interval)\n",
    "# test_data = create_lookback_features(test_data, lookback_window, prediction_interval)\n",
    "\n",
    "# # 6. Optional: Handle missing data (NaN) in the feature columns\n",
    "# train_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "# valid_data.fillna(method='ffill', inplace=True)\n",
    "# test_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# # 7. Optional: Scaling the features\n",
    "# scaler = StandardScaler()\n",
    "# features = ['close_open', 'high_low', 'close_open_highlow', 'high_max_open_close', 'volume_change', 'ma_close', 'ma_volume', 'price_change', 'volatility']\n",
    "# train_data[features] = scaler.fit_transform(train_data[features])\n",
    "# valid_data[features] = scaler.transform(valid_data[features])\n",
    "# test_data[features] = scaler.transform(test_data[features])\n",
    "\n",
    "# # 8. Feature Engineering - Adding the MultiIndex format as you requested\n",
    "# def add_multiindex_format(df):\n",
    "#     # Recompute the features in the format you provided\n",
    "#     features = ['close_open', 'high_low', 'close_open_highlow', 'high_max_open_close', 'high_max_open_close_highlow',\n",
    "#                 'high_minus_low', 'low_max_open_close', 'high_to_open', 'low_to_open', 'close_to_high', 'close_to_low',\n",
    "#                 'ma_close', 'ma_volume', 'volatility', 'price_change', 'ma_30_close', 'ma_60_close', 'ma_30_volume',\n",
    "#                 'std_close', 'std_volume']\n",
    "\n",
    "#     df_features = df[features]\n",
    "\n",
    "#     # Now, we'll change the column format to MultiIndex\n",
    "#     columns = pd.MultiIndex.from_tuples([('feature', col) for col in df_features.columns])\n",
    "\n",
    "#     # Assign this MultiIndex to the dataframe\n",
    "#     df_features.columns = columns\n",
    "\n",
    "#     return df_features\n",
    "\n",
    "# # Apply the MultiIndex formatting to the datasets\n",
    "# train_data = add_multiindex_format(train_data)\n",
    "# valid_data = add_multiindex_format(valid_data)\n",
    "# test_data = add_multiindex_format(test_data)\n",
    "\n",
    "# # 9. Save the processed datasets (after formatting)\n",
    "# train_data.to_csv('train_data_2020_2022.csv')\n",
    "# valid_data.to_csv('valid_data_2022.csv')\n",
    "# test_data.to_csv('test_data_2022_2024.csv')\n",
    "\n",
    "# # 10. Show the final processed data\n",
    "# print(\"Training Set:\")\n",
    "# print(train_data.head())\n",
    "\n",
    "# # 11. Check column names and datatypes\n",
    "# print(\"Training set columns: \")\n",
    "# print(train_data.columns)\n",
    "# print(\"Training set datatypes: \")\n",
    "# print(train_data.dtypes)\n",
    "\n",
    "# # 12. Check shape (len, width)\n",
    "# print(\"Training set shape: \")\n",
    "# print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make it multi-index for features and labels like the csi300 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# from scipy.stats import linregress\n",
    "\n",
    "# # 1. Define the date ranges for the new splits (2020-2024)\n",
    "# train_end = datetime(2022, 3, 31)  # Q1 2022 ends\n",
    "# valid_start = datetime(2022, 4, 1)  # Q2 2022 starts\n",
    "# valid_end = datetime(2022, 6, 30)  # Q2 2022 ends\n",
    "# test_start = datetime(2022, 7, 1)  # Q3 2022 starts\n",
    "# test_end = datetime(2024, 12, 31)  # Q4 2024 ends\n",
    "\n",
    "# # 2. Create train, validation, and test sets based on the new splits\n",
    "# train_data = data[data.index <= train_end]\n",
    "# valid_data = data[(data.index >= valid_start) & (data.index <= valid_end)]\n",
    "# test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
    "\n",
    "# # 3. Feature Engineering - Add CSI300-like features. CHOOSE 22 FEATURES\n",
    "# def calculate_csi300_features(df):\n",
    "#     # Basic features matching CSI300 format\n",
    "\n",
    "#     # Price Change Ratios\n",
    "#     df['(close-open)/open'] = (df['Close'] - df['Open']) / df['Open']\n",
    "#     df['(high-low)/open'] = (df['High'] - df['Low']) / df['Open']\n",
    "#     df['(2*close-high-low)/open'] = (2 * df['Close'] - df['High'] - df['Low']) / df['Open']\n",
    "#     print(\"Price Change Ratios done\")\n",
    "    \n",
    "#     # Moving Averages and Standard Deviations (Volatility)\n",
    "#     df['Mean(close,5)/close'] = df['Close'].rolling(window=5).mean() / df['Close']\n",
    "#     df['Mean(close,10)/close'] = df['Close'].rolling(window=10).mean() / df['Close']\n",
    "#     df['Mean(close,20)/close'] = df['Close'].rolling(window=20).mean() / df['Close']\n",
    "#     df['Std(close,5)/close'] = df['Close'].rolling(window=5).std() / df['Close']\n",
    "#     df['Std(close,10)/close'] = df['Close'].rolling(window=10).std() / df['Close']\n",
    "#     df['Std(close,20)/close'] = df['Close'].rolling(window=20).std() / df['Close']\n",
    "#     print(\"Moving Averages and Standard Deviations done\")\n",
    "\n",
    "#     # Relative Strength and Trends (Slope calculation)\n",
    "#     for window in [5, 10, 20]:\n",
    "#         df[f'Slope(close,{window})/close'] = df['Close'].rolling(window).apply(\n",
    "#             lambda x: linregress(range(len(x)), x).slope / x[-1] if len(x) == window else np.nan\n",
    "#         )\n",
    "#     print(\"Relative Strength and Trends done\")\n",
    "\n",
    "#     # Volume-Weighted Measures\n",
    "#     df['VWAP/close'] = (df['Volume'] * df['Close']).cumsum() / df['Volume'].cumsum() / df['Close']\n",
    "#     df['Mean(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).mean() / (df['Volume'] + 1e-12)\n",
    "#     df['Std(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).std() / (df['Volume'] + 1e-12)\n",
    "#     print(\"Volume-Weighted Measures done\")\n",
    "\n",
    "#     # Historical Close Ratios\n",
    "#     df['Ref(close,5)/close'] = df['Close'].shift(5) / df['Close']\n",
    "#     df['Ref(close,10)/close'] = df['Close'].shift(10) / df['Close']\n",
    "#     df['Ref(close,20)/close'] = df['Close'].shift(20) / df['Close']\n",
    "#     print(\"Historical Close Ratios done\")\n",
    "\n",
    "#     # Correlation Measures\n",
    "#     df['Corr(close,log(volume+1),5)'] = df['Close'].rolling(window=5).corr(np.log(df['Volume'] + 1))\n",
    "#     df['Corr(close/Ref(close,1),log(volume/Ref(volume,1)+1),5)'] = (\n",
    "#         (df['Close'] / df['Close'].shift(1)).rolling(window=5)\n",
    "#         .corr(np.log(df['Volume'] / df['Volume'].shift(1) + 1))\n",
    "#     )\n",
    "#     print(\"Correlation Measures done\")\n",
    "\n",
    "#     # Price Extremes and Ratios\n",
    "#     df['Max(high,5)/close'] = df['High'].rolling(window=5).max() / df['Close']\n",
    "#     df['Min(low,5)/close'] = df['Low'].rolling(window=5).min() / df['Close']\n",
    "#     print(\"Price Extremes and Ratios done\")\n",
    "\n",
    "#     # 6. Add target label (future reference)\n",
    "#     df['label'] = df['Close'].shift(-5) / df['Close'].shift(-1) - 1  # Ref($close, -5) / Ref($close, -1) - 1\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering to each dataset\n",
    "# train_data = calculate_csi300_features(train_data)\n",
    "# valid_data = calculate_csi300_features(valid_data)\n",
    "# test_data = calculate_csi300_features(test_data)\n",
    "\n",
    "# # 7. Optional: Handle missing data (NaN) in the feature columns\n",
    "# train_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "# valid_data.fillna(method='ffill', inplace=True)\n",
    "# test_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# # 8. Format the dataframe to MultiIndex with datetime and Time\n",
    "# def format_to_multiindex(df):\n",
    "#     # # Step 1: Convert 'Time' column to datetime format\n",
    "#     data['Time'] = pd.to_datetime(data['Time'], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "#     # # Step 2: Extract date part from 'Time' to create 'datetime' for grouping\n",
    "#     data['datetime'] = data['Time'].dt.date\n",
    "\n",
    "#     # Step 3: Remove the date part from 'Time' to keep only the time portion\n",
    "#     data['Time'] = data['Time'].dt.time\n",
    "\n",
    "#     # # Step 4: Set 'datetime' as the first level of the index and 'Time' as the second level\n",
    "#     data.set_index(['datetime', 'Time'], inplace=True)\n",
    "\n",
    "#     # # OLD OLD OLD OLD OLD OLD Ensure datetime is in the correct format and set it as index\n",
    "#     # df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "#     # # Set 'datetime' and 'Time' as MultiIndex\n",
    "#     # df['Time'] = df['Time'].astype(str)  # Ensure 'Time' is treated as string for consistency\n",
    "#     # df.set_index(['datetime', 'Time'], inplace=True)\n",
    "    \n",
    "#     # Define the MultiIndex for features\n",
    "#     columns = pd.MultiIndex.from_tuples([('feature', col) for col in df.columns if col not in ['datetime', 'Time', 'label']] + [('label', 'Ref($close, -5) / Ref($close, -1) - 1')])\n",
    "#     # Assign this MultiIndex to the dataframe\n",
    "#     df.columns = columns\n",
    "#     return df\n",
    "\n",
    "# # Apply MultiIndex formatting\n",
    "# train_data = format_to_multiindex(train_data)\n",
    "# valid_data = format_to_multiindex(valid_data)\n",
    "# test_data = format_to_multiindex(test_data)\n",
    "\n",
    "# # 9. Save the processed datasets\n",
    "# train_data.to_csv('train_data_2020_2022_NQ100.csv')\n",
    "# valid_data.to_csv('valid_data_2022_NQ100.csv')\n",
    "# test_data.to_csv('test_data_2022_2024_NQ100.csv')\n",
    "\n",
    "# # 10. Show the final processed data\n",
    "# print(\"Training Set:\")\n",
    "# print(train_data.head())\n",
    "\n",
    "# # 11. Check column names and datatypes\n",
    "# print(\"Training set columns: \")\n",
    "# print(train_data.columns)\n",
    "# print(\"Training set datatypes: \")\n",
    "# print(train_data.dtypes)\n",
    "\n",
    "# # 12. Check shape (len, width)\n",
    "# print(\"Training set shape: \")\n",
    "# print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude version\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def calculate_csi300_features(df, instrument_id='NQ100'):\n",
    "    \"\"\"Calculate features and reshape data into CSI300 format.\"\"\"\n",
    "    # Make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create features dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Price Change Ratios\n",
    "    features['(close-open)/open'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    features['(high-low)/open'] = (df['High'] - df['Low']) / df['Open']\n",
    "    features['(2*close-high-low)/open'] = (2 * df['Close'] - df['High'] - df['Low']) / df['Open']\n",
    "    print(\"Price Change Ratios done\")\n",
    "\n",
    "    # Moving Averages and Standard Deviations (Volatility)\n",
    "    features['Mean(close,5)/close'] = df['Close'].rolling(window=5).mean() / df['Close']\n",
    "    features['Mean(close,10)/close'] = df['Close'].rolling(window=10).mean() / df['Close']\n",
    "    features['Mean(close,20)/close'] = df['Close'].rolling(window=20).mean() / df['Close']\n",
    "    features['Std(close,5)/close'] = df['Close'].rolling(window=5).std() / df['Close']\n",
    "    features['Std(close,10)/close'] = df['Close'].rolling(window=10).std() / df['Close']\n",
    "    features['Std(close,20)/close'] = df['Close'].rolling(window=20).std() / df['Close']\n",
    "    print(\"Moving Averages and Standard Deviations done\")\n",
    "\n",
    "    # Relative Strength and Trends (Slope calculation)\n",
    "    for window in [5, 10, 20]:\n",
    "        features[f'Slope(close,{window})/close'] = df['Close'].rolling(window).apply(\n",
    "            lambda x: linregress(range(len(x)), x).slope / x[-1] if len(x) == window else np.nan\n",
    "        )\n",
    "    print(\"Relative Strength and Trends done\")\n",
    "\n",
    "    # Volume-Weighted Measures\n",
    "    features['VWAP/close'] = (df['Volume'] * df['Close']).cumsum() / df['Volume'].cumsum() / df['Close']\n",
    "    features['Mean(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).mean() / (df['Volume'] + 1e-12)\n",
    "    features['Std(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).std() / (df['Volume'] + 1e-12)\n",
    "    print(\"Volume-Weighted Measures done\")\n",
    "\n",
    "    # Historical Close Ratios\n",
    "    features['Ref(close,5)/close'] = df['Close'].shift(5) / df['Close']\n",
    "    features['Ref(close,10)/close'] = df['Close'].shift(10) / df['Close']\n",
    "    features['Ref(close,20)/close'] = df['Close'].shift(20) / df['Close']\n",
    "    print(\"Historical Close Ratios done\")\n",
    "\n",
    "    # Correlation Measures\n",
    "    features['Corr(close,log(volume+1),5)'] = df['Close'].rolling(window=5).corr(np.log(df['Volume'] + 1))\n",
    "    features['Corr(close/Ref(close,1),log(volume/Ref(volume,1)+1),5)'] = (\n",
    "        (df['Close'] / df['Close'].shift(1)).rolling(window=5)\n",
    "        .corr(np.log(df['Volume'] / df['Volume'].shift(1) + 1))\n",
    "    )\n",
    "    print(\"Correlation Measures done\")\n",
    "\n",
    "    # Price Extremes and Ratios\n",
    "    features['Max(high,5)/close'] = df['High'].rolling(window=5).max() / df['Close']\n",
    "    features['Min(low,5)/close'] = df['Low'].rolling(window=5).min() / df['Close']\n",
    "    print(\"Price Extremes and Ratios done\")\n",
    "         \n",
    "    # Create DataFrame from features with the same index as input data\n",
    "    feature_df = pd.DataFrame(features, index=df.index)\n",
    "    \n",
    "    # Add datetime (using the index date) and instrument columns\n",
    "    feature_df['datetime'] = feature_df.index.date\n",
    "    feature_df['instrument'] = instrument_id\n",
    "    \n",
    "    # Reorder columns to put datetime and instrument first\n",
    "    cols = ['datetime', 'instrument'] + [col for col in feature_df.columns if col not in ['datetime', 'instrument']]\n",
    "    feature_df = feature_df[cols]\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "def process_single_instrument(data, instrument_id, date_ranges):\n",
    "    \"\"\"Process data for a single instrument.\"\"\"\n",
    "    # Calculate features\n",
    "    processed_df = calculate_csi300_features(data, instrument_id)\n",
    "    \n",
    "    # Set datetime and instrument as index\n",
    "    processed_df.set_index(['datetime', 'instrument'], inplace=True)\n",
    "    \n",
    "    # Split data based on date ranges\n",
    "    train_data = processed_df[processed_df.index.get_level_values(0) <= date_ranges['train_end'].date()]\n",
    "    valid_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['valid_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['valid_end'].date())\n",
    "    ]\n",
    "    test_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['test_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['test_end'].date())\n",
    "    ]\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "# Debug: Print input data structure\n",
    "print(\"Input data structure:\")\n",
    "print(data.head())\n",
    "print(\"\\nInput data info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Define date ranges\n",
    "date_ranges = {\n",
    "    'train_end': datetime(2022, 3, 31),\n",
    "    'valid_start': datetime(2022, 4, 1),\n",
    "    'valid_end': datetime(2022, 6, 30),\n",
    "    'test_start': datetime(2022, 7, 1),\n",
    "    'test_end': datetime(2024, 12, 31)\n",
    "}\n",
    "\n",
    "# Process the data for NQ100\n",
    "instrument_id = 'NQ100'\n",
    "train_data, valid_data, test_data = process_single_instrument(data, instrument_id, date_ranges)\n",
    "\n",
    "# Save the processed datasets\n",
    "train_data.to_csv('train_data_2020_2022_NQ100.csv')\n",
    "valid_data.to_csv('valid_data_2022_NQ100.csv')\n",
    "test_data.to_csv('test_data_2022_2024_NQ100.csv')\n",
    "\n",
    "# Print sample output to verify format\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Print shape information\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Validation data: {valid_data.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save each set as .pkl files\n",
    "# # locally\n",
    "# train_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_train.pkl'\n",
    "# valid_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_valid.pkl'\n",
    "# test_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# # google drive for colab\n",
    "# train_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_train.pkl'\n",
    "# valid_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_valid.pkl'\n",
    "# test_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# onedrive when working at krach\n",
    "train_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_train.pkl'\n",
    "valid_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_valid.pkl'\n",
    "test_path = 'C:\\\\Users\\\\amirani\\\\ReMASTER\\\\data\\\\NQ100\\\\NQ100_dl_test.pkl'\n",
    "\n",
    "# Save train data\n",
    "with open(train_path, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "# Save validation data\n",
    "with open(valid_path, 'wb') as f:\n",
    "    pickle.dump(valid_data, f)\n",
    "\n",
    "# Save test data\n",
    "with open(test_path, 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "# Return the paths (optional)\n",
    "train_path, valid_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensure pkl files have content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude version\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def calculate_csi300_features(df, instrument_id='NQ100'):\n",
    "    \"\"\"Calculate features and reshape data into CSI300 format.\"\"\"\n",
    "    # Make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create features dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Price Change Ratios\n",
    "    features['(close-open)/open'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    features['(high-low)/open'] = (df['High'] - df['Low']) / df['Open']\n",
    "    features['(2*close-high-low)/open'] = (2 * df['Close'] - df['High'] - df['Low']) / df['Open']\n",
    "    print(\"Price Change Ratios done\")\n",
    "    \n",
    "    # Moving Averages and Standard Deviations (Volatility)\n",
    "    features['Mean(close,5)/close'] = df['Close'].rolling(window=5).mean() / df['Close']\n",
    "    features['Mean(close,10)/close'] = df['Close'].rolling(window=10).mean() / df['Close']\n",
    "    features['Mean(close,20)/close'] = df['Close'].rolling(window=20).mean() / df['Close']\n",
    "    features['Std(close,5)/close'] = df['Close'].rolling(window=5).std() / df['Close']\n",
    "    features['Std(close,10)/close'] = df['Close'].rolling(window=10).std() / df['Close']\n",
    "    features['Std(close,20)/close'] = df['Close'].rolling(window=20).std() / df['Close']\n",
    "    print(\"Moving Averages and Standard Deviations done\")\n",
    "\n",
    "    # Relative Strength and Trends (Slope calculation)\n",
    "    for window in [5, 10, 20]:\n",
    "        features[f'Slope(close,{window})/close'] = df['Close'].rolling(window).apply(\n",
    "            lambda x: linregress(range(len(x)), x).slope / x[-1] if len(x) == window else np.nan\n",
    "        )\n",
    "    print(\"Relative Strength and Trends done\")\n",
    "\n",
    "    # Volume-Weighted Measures\n",
    "    features['VWAP/close'] = (df['Volume'] * df['Close']).cumsum() / df['Volume'].cumsum() / df['Close']\n",
    "    features['Mean(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).mean() / (df['Volume'] + 1e-12)\n",
    "    features['Std(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).std() / (df['Volume'] + 1e-12)\n",
    "    print(\"Volume-Weighted Measures done\")\n",
    "\n",
    "    # Historical Close Ratios\n",
    "    features['Ref(close,5)/close'] = df['Close'].shift(5) / df['Close']\n",
    "    features['Ref(close,10)/close'] = df['Close'].shift(10) / df['Close']\n",
    "    features['Ref(close,20)/close'] = df['Close'].shift(20) / df['Close']\n",
    "    print(\"Historical Close Ratios done\")\n",
    "\n",
    "    # Correlation Measures\n",
    "    features['Corr(close,log(volume+1),5)'] = df['Close'].rolling(window=5).corr(np.log(df['Volume'] + 1))\n",
    "    features['Corr(close/Ref(close,1),log(volume/Ref(volume,1)+1),5)'] = (\n",
    "        (df['Close'] / df['Close'].shift(1)).rolling(window=5)\n",
    "        .corr(np.log(df['Volume'] / df['Volume'].shift(1) + 1))\n",
    "    )\n",
    "    print(\"Correlation Measures done\")\n",
    "\n",
    "    # Price Extremes and Ratios\n",
    "    features['Max(high,5)/close'] = df['High'].rolling(window=5).max() / df['Close']\n",
    "    features['Min(low,5)/close'] = df['Low'].rolling(window=5).min() / df['Close']\n",
    "    print(\"Price Extremes and Ratios done\")\n",
    "         \n",
    "    # Create DataFrame from features with the same index as input data\n",
    "    feature_df = pd.DataFrame(features, index=df.index)\n",
    "    \n",
    "    # Add datetime (using the index date) and instrument columns\n",
    "    feature_df['datetime'] = feature_df.index.date\n",
    "    feature_df['instrument'] = instrument_id\n",
    "    \n",
    "    # Reorder columns to put datetime and instrument first\n",
    "    cols = ['datetime', 'instrument'] + [col for col in feature_df.columns if col not in ['datetime', 'instrument']]\n",
    "    feature_df = feature_df[cols]\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "def process_single_instrument(data, instrument_id, date_ranges):\n",
    "    \"\"\"Process data for a single instrument.\"\"\"\n",
    "    # Calculate features\n",
    "    processed_df = calculate_csi300_features(data, instrument_id)\n",
    "    \n",
    "    # Set datetime and instrument as index\n",
    "    processed_df.set_index(['datetime', 'instrument'], inplace=True)\n",
    "    \n",
    "    # Split data based on date ranges\n",
    "    train_data = processed_df[processed_df.index.get_level_values(0) <= date_ranges['train_end'].date()]\n",
    "    valid_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['valid_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['valid_end'].date())\n",
    "    ]\n",
    "    test_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['test_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['test_end'].date())\n",
    "    ]\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "# Debug: Print input data structure\n",
    "print(\"Input data structure:\")\n",
    "print(data.head())\n",
    "print(\"\\nInput data info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Define date ranges\n",
    "date_ranges = {\n",
    "    'train_end': datetime(2022, 3, 31),\n",
    "    'valid_start': datetime(2022, 4, 1),\n",
    "    'valid_end': datetime(2022, 6, 30),\n",
    "    'test_start': datetime(2022, 7, 1),\n",
    "    'test_end': datetime(2024, 12, 31)\n",
    "}\n",
    "\n",
    "# Process the data for NQ100\n",
    "instrument_id = 'NQ100'\n",
    "train_data, valid_data, test_data = process_single_instrument(data, instrument_id, date_ranges)\n",
    "\n",
    "# Save the processed datasets\n",
    "train_data.to_csv('train_data_2020_2022_NQ100.csv')\n",
    "valid_data.to_csv('valid_data_2022_NQ100.csv')\n",
    "test_data.to_csv('test_data_2022_2024_NQ100.csv')\n",
    "\n",
    "# Print sample output to verify format\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Print shape information\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Validation data: {valid_data.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Convert 'Time' column to datetime format\n",
    "# data['Time'] = pd.to_datetime(data['Time'], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "# # Step 2: Check for any NA values and drop columns with NA values (if any)\n",
    "# data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# # Step 3: Perform robust daily Z-score normalization on each feature dimension\n",
    "# # Extract date component for daily grouping\n",
    "# data['Date'] = data['Time'].dt.date\n",
    "# # Group by 'Date' and normalize 'Open', 'High', 'Low', 'Close', and 'Volume' columns\n",
    "# feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "# data[feature_columns] = data.groupby('Date')[feature_columns].transform(zscore)\n",
    "\n",
    "# # Step 4: Drop 5% of the most extreme values from the 'Close' column to reduce label outliers\n",
    "# # Identify upper and lower 2.5% quantiles for 'Close'\n",
    "# q_low, q_high = data['Close'].quantile([0.025, 0.975])\n",
    "# data = data[(data['Close'] >= q_low) & (data['Close'] <= q_high)]\n",
    "\n",
    "# # Step 5: Drop NA rows (if any remain after filtering) and reset index\n",
    "# data.dropna(inplace=True)\n",
    "# data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Step 6: Save the cleaned data to a new CSV file\n",
    "# # Create the directory if it doesn't exist\n",
    "# save_path = 'new_data/reshaped_data.csv'\n",
    "# os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "# data.to_csv(save_path, index=False)\n",
    "\n",
    "# # Display first few rows of the cleaned data to confirm\n",
    "# data.head()\n",
    "\n",
    "# print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, valid, and test sets by datetime. Earlier years are in training, recent years in test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'data' is your original dataframe with columns like 'Date', 'Time', 'Open', 'High', 'Low', 'Close', etc.\n",
    "\n",
    "# # 1. Combine Date and Time into a single 'datetime' column\n",
    "# # Ensure that 'Date' is a string or datetime type and 'Time' is a string (e.g., \"HH:MM:SS\")\n",
    "\n",
    "# # Convert 'Date' to datetime (if not already in datetime format)\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# # Check the type of 'Time' column to handle it accordingly\n",
    "# if data['Time'].dtype == 'datetime64[ns]':  # If Time is already in datetime64[ns]\n",
    "#     data['Time'] = data['Time'].dt.time  # Extract the time part only\n",
    "# else:  # If Time is a string like 'HH:MM:SS'\n",
    "#     data['Time'] = pd.to_timedelta(data['Time'], errors='coerce')\n",
    "\n",
    "# # Combine 'Date' and 'Time' into a single 'datetime' column\n",
    "# data['datetime'] = data['Date'] + pd.to_timedelta(data['Time'].astype(str))\n",
    "\n",
    "# # Drop the original 'Date' and 'Time' columns if no longer needed\n",
    "# data.drop(columns=['Date', 'Time'], inplace=True)\n",
    "\n",
    "# # 2. Feature Engineering\n",
    "# data['close_open'] = (data['Close'] - data['Open']) / data['Open']\n",
    "# data['high_low'] = (data['High'] - data['Low']) / data['Open']\n",
    "# data['close_open_highlow'] = (data['Close'] - data['Open']) / (data['High'] - data['Low'] + 1e-12)\n",
    "# data['high_max_open_close'] = (data['High'] - data[['Open', 'Close']].max(axis=1)) / data['Open']\n",
    "\n",
    "# # 3. Handle NaN or infinite values\n",
    "# # Replace NaN with the column mean, or you can use other strategies\n",
    "# data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# # Replace any infinity values with a large number or NaN\n",
    "# data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# # Reapply fillna to handle any NaNs created by infinities\n",
    "# data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# # 4. Normalize the newly created features\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Select columns for scaling\n",
    "# features = ['close_open', 'high_low', 'close_open_highlow', 'high_max_open_close']\n",
    "# data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# # 5. Split the data into train, valid, and test\n",
    "# train_size = int(0.7 * len(data))  # 70% for training\n",
    "# valid_size = int(0.15 * len(data))  # 15% for validation\n",
    "# test_size = len(data) - train_size - valid_size  # 15% for testing\n",
    "\n",
    "# train_data = data[:train_size]\n",
    "# valid_data = data[train_size:train_size + valid_size]\n",
    "# test_data = data[train_size + valid_size:]\n",
    "\n",
    "# # 6. Save each set as .pkl files\n",
    "# train_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_train.pkl'\n",
    "# valid_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_valid.pkl'\n",
    "# test_path = '/Users/areejmirani/Desktop/ReMASTER/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# # Save train data\n",
    "# with open(train_path, 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "\n",
    "# # Save validation data\n",
    "# with open(valid_path, 'wb') as f:\n",
    "#     pickle.dump(valid_data, f)\n",
    "\n",
    "# # Save test data\n",
    "# with open(test_path, 'wb') as f:\n",
    "#     pickle.dump(test_data, f)\n",
    "\n",
    "# # Return the paths (optional)\n",
    "# train_path, valid_path, test_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure data is in the .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to load and test if data exists in each .pkl file\n",
    "# def test_data_in_pkl(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             data = pickle.load(f)\n",
    "#         # Check if the DataFrame is not empty\n",
    "#         if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "#             print(f\"Data exists in {file_path} and has {len(data)} rows and {len(data.columns)} columns.\")\n",
    "#         else:\n",
    "#             print(f\"File {file_path} is either empty or not a DataFrame.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# # Test each .pkl file\n",
    "# test_data_in_pkl(train_path)\n",
    "# test_data_in_pkl(valid_path)\n",
    "# test_data_in_pkl(test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
